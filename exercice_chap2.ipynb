{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' 1. Lorem Ipsum is just a random txt that devs use as a placeholder for multiple things (especially web developping) \n",
    "when you don't have the real text and just want to test your functionnality. Put a [Lorem Ipsum](https://www.lipsum.com/)\n",
    "of 3 paragraphs in a txt file using python, each paragraph delimited by two new line.'''\n",
    "\n",
    "import lorem \n",
    "\n",
    "with open('datachap2/lorem.txt', 'a+') as f:\n",
    "    for i in range(3):\n",
    "        f.write(lorem.paragraph())\n",
    "        f.write('\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Update the txt file by removing the first paragraph.\n",
    "with open('datachap2/lorem.txt', 'r') as f:\n",
    "    paragraphs = f.readlines()\n",
    "    paragraphs.pop(0)\n",
    "\n",
    "with open('datachap2/lorem.txt', 'w') as f:\n",
    "    [f.write(paragraph) for paragraph in paragraphs]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'authors': [['Yann LeCun', 'Y. Bengio', 'Geoffrey Hinton'], ['Ian Goodfellow', 'Jean Pouget-Abadie', 'Mehdi Mirza', 'Bing Xu', 'David Warde-Farley', 'Sherjil Ozair', 'Aaron Courville', 'Yoshua Bengio']], 'title': ['Deep Learning', 'Generative Adversarial Networks'], 'affiliations': [['Facebook AI Research', 'University of Montréal', ' University of Toronto'], ['Universite de Montréal', 'Ecole Polytechnique', 'Universite de Montréal', 'Universite de Montréal', 'Universite de Montréal', 'Indian Institute of Technology Delhi', 'Universite de Montréal', 'CIFAR Senior Fellow']]}\n"
     ]
    }
   ],
   "source": [
    "''' 3.  Create a dict from the paper of [lecun et al.](https://www.researchgate.net/publication/277411157_Deep_Learning) \n",
    "and [goodfellow et al.](https://arxiv.org/abs/1406.2661) with authors, title, affiliations.'''\n",
    "\n",
    "paper = {\n",
    "    'authors': (['Yann LeCun','Y. Bengio','Geoffrey Hinton'],['Ian Goodfellow','Jean Pouget-Abadie','Mehdi Mirza','Bing Xu','David Warde-Farley','Sherjil Ozair','Aaron Courville','Yoshua Bengio']),\n",
    "    'title': ['Deep Learning','Generative Adversarial Networks'],\n",
    "    'affiliations': (['Facebook AI Research','University of Montréal',' University of Toronto'],['Universite de Montréal','Ecole Polytechnique','Universite de Montréal','Universite de Montréal','Universite de Montréal','Indian Institute of Technology Delhi','Universite de Montréal','CIFAR Senior Fellow'])\n",
    "}\n",
    "\n",
    "# 4.Save the previously created dict in the JSON format and load it back.\n",
    "\n",
    "import json\n",
    "with open('datachap2/paper.json', 'w') as f:\n",
    "    json.dump(paper,f)\n",
    "\n",
    "with open('datachap2/paper.json', 'r') as f:\n",
    "    paper_load = json.load(f)\n",
    "\n",
    "print(paper_load)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Save the previously created dict in the pickle format. Try to open manually (i.e with a text editor), is it human readable ?\n",
    "import pickle\n",
    "with open('datachap2/paper.pkl', 'wb') as f: # 'wb' signifie que ecrire en mode binaire\n",
    "    pickle.dump(paper,f)\n",
    "\n",
    "''' non, le fichier pickle n'est pas lisible par un humain, l'editeur de texte ne peut pas l'ouvrir. '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<note>\n",
      "  <date>2015-09-01</date>\n",
      "  <hour>08:30</hour>\n",
      "  <to>Tove</to>\n",
      "  <from>Jani</from>\n",
      "  <body>Don't forget me this weekend!</body>\n",
      "</note>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 6. Parse the xml_file2 in the same way as seen in the lecture\n",
    "import lxml.etree \n",
    "xml_file = 'datachap2/xml_file2.nxml'\n",
    "root = lxml.etree.parse(xml_file)\n",
    "print(lxml.etree.tostring(root, encoding=\"unicode\", pretty_print=True)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'date': ['2015-09-01'], 'hour': ['08:30'], 'body': [\"Don't forget me this weekend!\"], 'to': ['Tove'], 'from': ['Jani']}\n"
     ]
    }
   ],
   "source": [
    "#  put infos in a dict and save it in a json file.\n",
    "dict_xml = {'date': root.xpath(\"//date//text()\"),\n",
    "            'hour': root.xpath(\"//hour//text()\"),\n",
    "            'body': root.xpath(\"//body//text()\"),\n",
    "            'to': root.xpath(\"//to//text()\"),\n",
    "            'from': root.xpath(\"//from//text()\")}\n",
    "print(dict_xml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('datachap2/dict_xml.json', 'w') as f :\n",
    "    json.dump(dict_xml, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Download an image of your choice and save it in either jpg or png.\n",
    "from PIL import Image\n",
    "import requests as re\n",
    "\n",
    "image = Image.open(re.get(\"https://ecogestion.unistra.fr/websites/ecogestion/_Logos_/fac_-_unistra/Faculte_Gestion_Large_Couleur.png\",stream=True).raw)\n",
    "image.save('datachap2/fseg_logo.png','png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['@type', 'title', 'accessLevel', 'accrualPeriodicity', 'contactPoint', 'description', 'identifier', 'keyword', 'modified', 'publisher', 'spatial', 'temporal', 'distribution', 'bureauCode', 'programCode', 'language'])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'org:Organization'}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 8.From the data/Chap2/data_world.json file, create a set of publisher type.\n",
    "with open('datachap2/data_world.json','r', encoding= \"utf-8\") as f :\n",
    "    data = json.load(f)\n",
    "print(data[0].keys())\n",
    "\n",
    "publisher_type = [d[\"publisher\"]['@type'] for d in data]\n",
    "set(publisher_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9.From the data/Chap2/data_world.json file, delete the key of your choice and save the new dict as data_world_cleaned.json.\n",
    "\n",
    "with open('datachap2/data_world.json','r') as f:\n",
    "    data = json.load(f)\n",
    "    key_to_delete = list(data[0].keys())[0]\n",
    "    for item in data:\n",
    "        del item[key_to_delete]\n",
    "\n",
    "with open('datachap2/data_world_cleaned.json','w') as f:\n",
    "    json.dump(data,f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10. From the data/Chap2/data_world.json file, create the co-occurence matrix between \"accessLevel\" and \"accrualPeriodicity\".\n",
    "with open('datachap2/data_world.json','r') as f:\n",
    "    data = json.load(f)\n",
    "    al = [d['accessLevel'] for d in data]\n",
    "    ap = [d[\"accrualPeriodicity\"] for d in data]\n",
    "\n",
    "from collections import Counter\n",
    "co_occurence = Counter(zip(al,ap))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
